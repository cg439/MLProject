%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{enumitem}
% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2012} with
% \usepackage[nohyperref]{icml2012} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2012} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2012}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2012}



\begin{document} 

\twocolumn[
\icmltitle{Sentiment Extraction of Earbud Amazon Reviews }

\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\section{Introduction}
Amazon.com provides an abundant amount of reviews to aid buyers on their decision to purchase the product, but it is very time consuming to read and filter through all the reviews for relevant information.
To solve this issue, we propose to automatically extract sentiments from the entire set of reviews and aggregate it in a concise summary.

Our basic approach is to generate a word bank of relevant terms using LDA from the product descriptions and questions asked by purchasers on the product. Next, using SVM and SentiWordNet with Word Sense Disambiguation (WSD), annotated all sentences with a sentiment score and only use the non-neutral sentences in the n-gram generator to produce relevant adj-noun bigrams or trigrams. These terms will be filtered by the word bank produced by the LDA and ranked to produce a set of most relevant terms with their sentiment. The output will be the sentences these terms were produced from.  

\section{Problem Definition and Methods} 
 \subsection{Task Definition}
Our problem is to extract sentiment tagged aspects that will give a good representation of the text corpus it was generated from using the method described below. The main questions we want to answer are:
\begin{enumerate}
\item What are characteristics of products important to consumers?
\item What are important features reviewers focus on?
\item What makes review content irrelevant or less relevant?
\end{enumerate}

It is an important problem, because if our algorithm accurately extracts the sentiment, we will be saving consumers lots of time on not just Amazon.com but other large marketplaces. Furthermore, this can be applied to other areas involving text summarization, such as automated generation of catchphrases and key points for company ads.
\subsection{Algorithm and Methods}
To extract relevant terms from the reviews, each sentence of the reviews was made into a POS-tagged parse tree and chunked on various grammars. Through multiple runs of chunking on different grammars, it was determined that chunking into bigrams of $<$adj$>$*$<$noun$>$* and trigrams of $<$DT$>$?$<$adj$>$*$<$noun$>$* were most accurate in extracting relevant terms. To offset the thousands of false positives the parse tree generated, the results of the parse tree were filtered by the word bank generated by the LDA. If the word existed in the LDA, it would be given heavier weight. At the end, only words with a weight above a certain threshold were considered relevant terms. \\\\
To give each sentence a sentiment score, we first train a bag of words with a combination of the movie review corpus from nltk and a portion of the annotated laptop reviews from Stanford SNAP library with all the unique words stemmed by using Porter Stemmer. With an interface to SentiWordNet using the NLTK WordNet classes, for each sentence, we split it into individual words and for each word in that sentence, we look it up in the SentiWordNet dictionary for its positive and negative sentiment scores. In order to obtain more accurate results, we also used WSD with finding the closest meaning of that particular word by determining the similarity that word has with the other words in the same sentence.  The sentiment score of a sentence would be the summation of the sentiment scores of the words inside that sentence. If a word does not exist in the bag of words we obtained from training, it will simply be ignored.


\section{Experimental Evaluation} 
\subsection{Methodology}
\begin{enumerate}
\item Preprocessing stage: 
\newline Using the scraper extract all user reviews and split by sentence to generate a list of sentences from all the reviews per product.

\item Processing stage
	\begin{enumerate} [label* = \arabic*.]
	\item Determining Important Features of each product \\
	This will be done with a combined static and dynamic technique.
		\begin{enumerate} [label* = \arabic*.]
	\item Static Technique \\
	 Generate important aspects from product descriptions scraped from Amazon.com by using a TFIDF weighting scheme from NLTK. These words, along with the words sound, audio, comfort, bass, highs, mids, tones, durability, quality, blocking, noise, sharp, deep, and tangle will be considered important aspects automatically.
	\item Dynamic Technique \\
	 Using a parse tree split each sentence into bigrams and trigrams with the grammar format of $<$adj$><$noun$>$ to represent candidate product aspects of importance to reviewers. We can filter this list using stopword filters and other grammatical based rules to get rid of false positives. We can create an inverted index of these terms and the sentences they occurred in. We can also easily use the list of sentences to find the frequency of aspects in the review corpus.
	\end{enumerate}
		\item Using SVM or Naive Bayes annotate each sentence with a sentiment score based on the previously created list of relevant features for the product. If a sentence has a score below a low threshold then it will be discarded.
	\end{enumerate}
\item Post-processing stage: Aggregation
	\begin{enumerate}[label* = \arabic*.]
	\item We need to summarize the aspect information we have and present sentences from the corpus that best represent the corpus. One way to do this is to rank the aspects based on frequency and then calculate an averaged sentiment score, which can be used to determine which sentiment polarity the majority of the reviews have with respect to that aspect. We can then take a sentence to represent this aspect. A simple scheme could be to find the sentence with the greatest magnitude sentiment score with the same polarity as the aspect average score.
	\end{enumerate}
\end{enumerate}
Evaluation: Compare the results with annotated libraries of training and testing data from a project on reviews to find our accuracy on tagging aspect features and sentiment intensity/polarity.
After that we will have to do manual surveying from people of the different summarization techniques that we can use.
\newline \\
To evaluate our results, we ran the different parts of our algorithm on laptop review data that was annotated with relevant terms per sentence and each of those terms was annotated with either “positive”, 
“negative”, or “neutral” sentiment. Since we wanted to get the overall sentiment of the sentence, each sentence was given a sentiment score based on majority vote of the sentiment of the terms extracted from it. The testing and training data are realistic, because it is also technology review data that focused on extraction of relevant terms and sentiment as a mean of text summarization.  

Each part of the algorithm was run separately on the data to get accuracy of the parts. We assume that if the parts are accurate, the overall goal of extracting relevant terms with sentiment will be accurate as well.
The preliminary algorithm was (Chetan write stuffs ). 
\newline 
The algorithm for extracting relevant terms from the parse tree was run multiple times using different grammars to chunk on in order to determine which grammar produced the most accurate results. In general, the tree chunked on adj,noun bigrams or determiner,adj,noun trigrams. The results are summarized below: 


\subsection{Results}
\subsection{Discussion}
The high amount of false positives in term selection via parse tree chunking on bigrams and trigrams is due to the oversensitivity of the chunker. The parse tree will chunk on any phrase that fulfills the grammar, including terms such as “father” or “mother”, which are nouns but irrelevant to the reviews.
\section{Related Work}
We will restrict our focus to one type of product (headphones) to make evaluation easier. We see 3 main stages to this project:



\section{Future Work}
\section{Conclusion}
\section{Resources}

Python libraries: Sci-Kit Learn, NLTK, Numpy, Scipy

Data for benchmarking and testing accuracy

\begin{itemize}
\item Web scraper written in Node.js to parse relevant data (reviews, product descriptions, etc)
\newline 

\item Annotated data from SemEval2014-Task4 on aspect feature based sentiment analysis
\newline
\url{http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools}

\item Amazon product review data mined by Stanford
\newline
\url{http://snap.stanford.edu/data/web-Amazon-links.html}
\end{itemize}

Readings on Parsing \& Sentiment Analysis:
\begin{itemize}
\item Stanford paper on parsing \& sentiment analysis:
\newline
\url{http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}

\item Multi-aspect sentiment analysis
\newline 
\url{http://www.cs.cornell.edu/home/cardie/papers/masa-sentire-2011.pdf}

\item Catching the Drift: Probabilistic Content Models, with Applications to
Generation and Summarization \newline
\url{http://www.aclweb.org/anthology/N04-1015}
\end{itemize}

\section{Schedule}

By November $16^{th}$, complete:
\begin{itemize}
\item the web scraper to scrape product descriptions
\item the static technique of the processing stage 
\item the annotation of sentences with SVM or Naive Bayes 
\end{itemize}
By November $22^{nd}$, complete:
\begin{itemize}
\item the dynamic technique of the processing stage
\end{itemize}
By November $29^{th}$, complete the post-processing aggregation stage.
\newline \\
By December $4^{th}$, the due date of the poster, complete:
\begin{itemize}
\item 
ranking of sentiments in terms of relevance to consumer requirements 
\item charts and tables of data
\item begin writing the final report 
\end{itemize}

Final report due: December $10^{th}$
\end{document}