%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{enumitem}
% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2012} with
% \usepackage[nohyperref]{icml2012} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2012} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2012}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2012}



\begin{document} 

\twocolumn[
\icmltitle{CS4780 Project Proposal }

\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\section{Size of Team}
Size of team is four people. 

\section{Motivation} 
 
Before making a purchase on Amazon.com, buyers typically read reviews of the product to determine if it will meet their requirements. Although reviews are a great resource, it is very time consuming to read through many to find the information that is relevant to buyers. Furthermore, buyers must filter out a large number of unhelpful reviews. Although Amazon attempts to ease the problem by ranking its reviews in order of helpfulness, the top reviews may not address all the product features that the buyer is interested in, may be too old to still be reliable, or may be considered a top review simply due to visibility. 

To solve this issue, we propose to automatically extract sentiments from the entire set of reviews and aggregate it in a concise summary.

\section{Statement of Problem and Key Questions} 

Using user reviews, help consumers easily and accurately understand the important sentiments contained in the reviews. 

Key Questions:

\begin{enumerate}
\item What are the characteristics of the product that reviewers find important?
\item How do reviewers rate these features?
\end{enumerate}

\section{General Approach}
We will restrict our focus to one type of product (headphones) to make evaluation easier. We see 3 main stages to this project:

\begin{enumerate}
\item Preprocessing stage: Extract relevant product features
\newline 
	\begin{enumerate} [label* = \arabic*.]
	\\
	\item Using the scraper extract all user reviews and split by sentence to generate a list of sentences from all the reviews per product.
	\end{enumerate}
\item Processing stage
	\begin{enumerate} [label* = \arabic*.]
	\item Using SVM or Naive Bayes annotate each sentence with a sentiment score based on the previously created list of relevant features for the product. If a sentence has a score below a low threshold then it will be discarded.
	\item In order to determine if a review contains relevant features, we will use a combined static and dynamic technique to extract the aspect features. The static method will utilize product specifications extracted from the product descriptions of headphones products using a web scraper. Using a TFIDF weighting scheme from NLTK we will generate a list of words that have high relevancy to headphones products. These, in addition to the words sound, audio, comfort, base, highs, mids, tones, durability, quality, blocking, noise, sharp, deep, and tangle will be considered important aspects automatically. The dynamic method will utilize certain rule based approaches including checking for bigrams that are <adj><noun> pairs.Using a parse tree split each sentence into bigrams and trigrams with the grammar format of $<$adj$><$noun$>$ to represent candidate product aspects of importance to reviewers. We can filter this list using stopword filters and other grammatical based rules to get rid of false positives. We can create an inverted index of these terms and the sentences they occurred in. We can also easily use the list of sentences to find the frequency of aspects in the review corpus.
	\end{enumerate}
\item Post-processing stage: Aggregation
	\begin{enumerate}[label* = \arabic*.]
	\item We need to summarize the aspect information we have and present sentences from the corpus that best represent the corpus. One way to do this is to rank the aspects based on frequency and then calculate an averaged sentiment score, which can be used to determine which sentiment polarity the majority of the reviews have with respect to that aspect. We can then take a sentence to represent this aspect. A simple scheme could be to find the sentence with the greatest magnitude sentiment score with the same polarity as the aspect average score.
	\end{enumerate}
\end{enumerate}
Evaluation: Compare the results with annotated libraries of training and testing data from a project on reviews to find our accuracy on tagging aspect features and sentiment intensity/polarity.
After that we will have to do manual surveying from people of the different summarization techniques that we can use.
\section{Progress}
\begin{enumerate}
\item Refinement of Problem and General Approach
	\begin{enumerate} [label* = \arabic*.]
	\item We have decided to change the problem and our approach to it to focus more on the text summarization aspect of the project rather than the dashboard GUI. We have refined our project to be an attempt to make a framework to summarize the Amazon reviews with respect to the features of the reviewed product. We no longer focus on the ability to compare other brands. We specifically have chosen headphones to shrink the scope of the problem and focus on one specific product area. Given a corpus of reviews for a headphone product, our framework will be able to output a list of sentences that best summarize the sentiment in the reviews towards the product and its features.
	\item Our new approach adapts a technique described in a 2008 paper about using a lexicon based system for summarizing review sentiment in relation to local services such as restaurants and hotels. To improve on this technique, we will also be doing feature engineering to improve the accuracy using prior mined information about the product. This takes place in the pre-processing stage described above, in which we generate relevant features from product descriptions and prior knowledge about headphones.
	\end{enumerate}
\item Project Progress \\
	\begin{enumerate} [label* = \arabic*.]
	\item Data collection \\
	The scraper is operational for collecting Amazon review data for any given product. We have already gathered 3,152 reviews and have parsed them into sentences, broken them up into tokens and tagged each token with part of speech. The scraper is currently being updated to intake product descriptions to generate the relevant features using the frequency distribution of words in the combined product descriptions. For analytical purposes, we will be copy and pasting directly into the program for parsing, tagging, and finding the frequency distribution until the scraper is updated. 
	\end{enumerate} 
\item Problems 
	\begin{enumerate} [label* = \arabic*.]
	\item Review Syntax \\
	Reviews may contain misspelled words, slang, or contractions that must be escaped. To resolve this issue, we will be using NLP's similar words function and slang libraries to match up the words with associated relevant features. To escape contractions, the text will be pre-processed before being parsed.
	\item Lack of Product Description \\
	If an item has no product description, we plan to simply parse its name and give it a generic list of words (as listed in the general approach) as its important features.
	\item 
	\end{enumerate}
\end{enumerate}

\section{Resources}

Python libraries: Sci-Kit Learn, NLTK, Numpy, Scipy

Data for benchmarking and testing accuracy

\begin{itemize}
\item Web scraper written in Node.js to parse relevant data (reviews, product descriptions, etc)
\newline 

\item Annotated data from SemEval2014-Task4 on aspect feature based sentiment analysis
\newline
\url{http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools}

\item Amazon product review data mined by Stanford
\newline
\url{http://snap.stanford.edu/data/web-Amazon-links.html}
\end{itemize}

Readings on Parsing \& Sentiment Analysis:
\begin{itemize}
\item Stanford paper on parsing \& sentiment analysis:
\newline
\url{http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}

\item Multi-aspect sentiment analysis
\newline 
\url{http://www.cs.cornell.edu/home/cardie/papers/masa-sentire-2011.pdf}

\item Catching the Drift: Probabilistic Content Models, with Applications to
Generation and Summarization \newline
\url{http://www.aclweb.org/anthology/N04-1015}
\end{itemize}

\section{Schedule}

By December $4^{th}$, the due date of the poster, complete:
\begin{itemize}
\item aggregate sentiments using the dashboard to deal with conflicting sentiments for same feature by aggregating and displaying them. 

For instance: If 4 people say they like the texture of a phone case, but 6 people say it slips around too much, display 40\% like texture, 60\% dislike texture. 

\item 
rank sentiments in terms of relevance to consumer requirements (can use combination of either product description, weight from all the reviews that expressed the sentiment, or an alternative method to relate the sentiment back to corpus of reviews and use the frequency to determine rank)

\end{itemize}

Final report due: December $10^{th}$
\end{document}